# 114-1 Econometrics (I) â€” Homework Assignments  
TA : **å¼µæ–‡èª **  

æœ¬å°ˆæ¡ˆæ”¶éŒ„ 114-1 å­¸æœŸã€Šè¨ˆé‡ç¶“æ¿Ÿå­¸ï¼ˆä¸€ï¼‰ã€‹å„æ¬¡ä½œæ¥­çš„é¡Œç›®èªªæ˜èˆ‡ç¤ºç¯„ç¨‹å¼ã€‚

---

## ğŸ“˜ Homework 1 â€” Variance and Covariance using the \( M_i \) Matrix  

### é¡Œç›®èªªæ˜
Students are asked to compute:  
1. The numerator of the **variance** of \( X \)  
2. The numerator of the **covariance** between \( X \) and \( Y \)  

using the definition based on the centering matrix  
\[
M_i = I - \frac{1}{N}\mathbf{1}\mathbf{1}^T
\]

### è¦æ±‚
- Use **R** to perform the calculations.  
- Include clear **comments** explaining each step.  
- Do **not** directly use `var()` or `cov()`.  
- Verify that  
  \[
  X'M_iY = \sum_i (X_i - \bar{X})(Y_i - \bar{Y})
  \]  
  and confirm consistency with built-in functions.

---

## ğŸ“— Homework 2 â€” Eigenvalues and Eigenvectors  

### é¡Œç›®èªªæ˜
Let  
\[
A = 
\begin{bmatrix}
1 & 2 & 3\\
2 & 9 & 6\\
3 & 6 & 7
\end{bmatrix}
\]

Tasks:  
1. Find the **eigenvalues** and **eigenvectors** of \( A \).  
2. Verify that:  
   - \( \mathrm{tr}(A) = \sum \text{eigenvalues} \)  
   - \( \det(A) = \prod \text{eigenvalues} \)  
   - The eigenvectors are **orthonormal**.  
3. Determine whether \( A \) is a **full-rank** matrix.

---

## ğŸ“˜ Homework 3 â€” Square Root of a Matrix  

### é¡Œç›®èªªæ˜
Given the matrix  
\[
A =
\begin{bmatrix}
5 & 2 \\
1 & 4
\end{bmatrix}
\]

Tasks:  
1. Use eigen decomposition to express  
   \[
   A = X \Lambda X^{-1}
   \]  
2. Construct  
   \[
   \Lambda^{1/2} = \operatorname{diag}(\sqrt{\lambda_1}, \sqrt{\lambda_2})
   \]  
3. Compute the matrix square root  
   \[
   A^{1/2} = X \Lambda^{1/2} X^{-1}
   \]  
4. Verify that  
   \[
   A^{1/2}A^{1/2} = A
   \]

---

## ğŸ“— Homework 4 â€” Create a 3Ã—3 Positive Definite Matrix with Negative Elements  

### é¡Œç›®èªªæ˜  
Design a **3Ã—3 matrix** that satisfies the following two conditions:

1. The matrix must **contain at least one negative element**.  
2. The matrix must be **positive definite**.

---

## ğŸ“˜ Homework 5 â€” OLS Using Matrix Algebra  

### é¡Œç›®èªªæ˜
Given a dataset containing  
\[
Y,\; \text{constant},\; T,\; G,\; R,\; P,
\]  
students must use **matrix algebra** to verify the core algebraic properties of Ordinary Least Squares (OLS).

Tasks:  
1. Compute the OLS estimator  
   \[
   \hat{\beta} = (X'X)^{-1} X'Y
   \]  
2. Compute the residual vector  
   \[
   e = Y - X\hat{\beta}
   \]  
3. Verify the orthogonality condition  
   \[
   X'e = 0
   \]  
4. Verify that residuals sum to zero  
   \[
   \sum_i e_i = 0
   \]  
   (This holds because the regression includes a constant term.)

### è¦æ±‚
- All calculations must be performed using **matrix operations in R** (`t()`, `%*%`, `solve()`).  
- Do **not** use `lm()` except for verification.  
- Final output must include:  
  - Î²Ì‚ å‘é‡  
  - æ®˜å·®å‘é‡ e  
  - é©—è­‰ \(X'e = 0\)  
  - é©—è­‰ \(\sum e = 0\)





